{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET='hohukelkazan'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read\n",
    "traindays = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('gs://{}/flights/trainday.csv'.format(BUCKET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark SQLビューに変換\n",
    "traindays.createOrReplaceTempView('traindays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スキーマ定義(TAXI_OUTも数値型にする)\n",
    "from pyspark.sql.types import StringType, FloatType, StructType, StructField\n",
    "\n",
    "header = 'FL_DATE,UNIQUE_CARRIER,AIRLINE_ID,CARRIER,FL_NUM,ORIGIN_AIRPORT_ID,ORIGIN_AIRPORT_SEQ_ID,ORIGIN_CITY_MARKET_ID,ORIGIN,DEST_AIRPORT_ID,DEST_AIRPORT_SEQ_ID,DEST_CITY_MARKET_ID,DEST,CRS_DEP_TIME,DEP_TIME,DEP_DELAY,TAXI_OUT,WHEELS_OFF,WHEELS_ON,TAXI_IN,CRS_ARR_TIME,ARR_TIME,ARR_DELAY,CANCELLED,CANCELLATION_CODE,DIVERTED,DISTANCE,DEP_AIRPORT_LAT,DEP_AIRPORT_LON,DEP_AIRPORT_TZOFFSET,ARR_AIRPORT_LAT,ARR_AIRPORT_LON,ARR_AIRPORT_TZOFFSET,EVENT,NOTIFY_TIME'\n",
    "\n",
    "def get_structfield(colname):\n",
    "  if colname in ['ARR_DELAY', 'DEP_DELAY', 'DISTANCE', 'TAXI_OUT']:\n",
    "    return StructField(colname, FloatType(), True)\n",
    "  else:\n",
    "    return StructField(colname, StringType(), True)\n",
    "  \n",
    "schema = StructType([get_structfield(colname) for colname in header.split(',')])\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = 'gs://{}/flights/tzcorr/all_flights-00000-*'.format(BUCKET) # 1/30th\n",
    "#inputs = 'gs://{}/flights/tzcorr/all_flights-*'.format(BUCKET)\n",
    "flights_csv = spark.read\\\n",
    "            .schema(schema)\\\n",
    "            .csv(inputs)\n",
    "# tmpビュー作成\n",
    "flights_csv.createOrReplaceTempView('flights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "spark.sql(\"SELECT * from traindays LIMIT 5\").show()\n",
    "spark.sql(\"SELECT * from flights LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train data\n",
    "traindayquery = \"\"\"\n",
    "select\n",
    "  f.*\n",
    "from flights f\n",
    "join traindays t\n",
    "on f.FL_DATE == t.FL_DATE\n",
    "where\n",
    "  t.is_train_day == 'True'\n",
    "\"\"\"\n",
    "traindata = spark.sql(traindayquery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check: 'ARR_DELAY', 'DEP_DELAY', 'DISTANCE', 'TAXI_OUT'がfloatになっている\n",
    "print(traindata.head(1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe()メソッドは列ごとの統計情報を計算して、show()はその結果を表示するよ\n",
    "# NULLはカウントしないからcountに差がでるよ(ex. スケジュールされたけど出発しなかった場合など)\n",
    "traindata.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# じゃあNULLを除外して特殊なフライトをなくそう\n",
    "traindayquery = \"\"\"\n",
    "select\n",
    "  f.*\n",
    "from flights f\n",
    "join traindays t\n",
    "on f.FL_DATE == t.FL_DATE\n",
    "where\n",
    "  t.is_train_day == 'True' and\n",
    "  f.dep_delay is not NULL and\n",
    "  f.arr_delay is not NULL\n",
    "\n",
    "\"\"\"\n",
    "traindata = spark.sql(traindayquery)\n",
    "traindata.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -> すべてのcount数が同じになった\n",
    " ただ根本解決にはなってない\n",
    " -> キャンセル、迂回が発生したかを表す絡むを見てレコードを絞った方が確実"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select\n",
    "  cancelled, diverted\n",
    "from flights\n",
    "limit 10\n",
    "\"\"\"\n",
    "spark.sql(query).show()\n",
    "\n",
    "query = \"\"\"\n",
    "select\n",
    "  cancelled, diverted\n",
    "from flights\n",
    "where\n",
    "  cancelled != '0.00'\n",
    "limit 10\n",
    "\"\"\"\n",
    "spark.sql(query).show()\n",
    "\n",
    "query = \"\"\"\n",
    "select\n",
    "  cancelled, diverted\n",
    "from flights\n",
    "where\n",
    "  diverted != '0.00'\n",
    "limit 10\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "どうやらキャンセルされたか、迂回されたかは0,1で格納されているみたい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# もっとしっかり取り除こう\n",
    "traindayquery = \"\"\"\n",
    "select\n",
    "  f.*\n",
    "from flights f\n",
    "join traindays t\n",
    "on f.FL_DATE == t.FL_DATE\n",
    "where\n",
    "  t.is_train_day == 'True' and\n",
    "  f.cancelled == '0.00' and\n",
    "  f.diverted == '0.00'\n",
    "\"\"\"\n",
    "traindata = spark.sql(traindayquery)\n",
    "traindata.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NULLを除外した時と同じ結果が得られた"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# トレーニング\n",
    "\n",
    "トレーニングデータの各レコードは、LabeldedPointクラスに変換する必要がある\n",
    "- https://spark.apache.org/docs/latest/api/java/org/apache/spark/ml/classification/LogisticRegressionModel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_example(fields):\n",
    "  return LabeledPoint(\\\n",
    "           float(fields['ARR_DALAY'] < 15), # on-time? \\\n",
    "           [\\\n",
    "             fields['DEP_DELAY'], \\\n",
    "             fields['TAXI_OUT'], \\\n",
    "             fields['DISTANCE'], \\\n",
    "           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーニング用のラベルと入力変数だけを取り出したデータ\n",
    "examples = traindata.rdd.map(to_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニング実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intercept: 切片\n",
    "lrmodel = LogisticRegressionWithLBFGS.train(examples, intercept=True)\n",
    "print(lrmodel.weights, lrmodel.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
