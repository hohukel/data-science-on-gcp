{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "PROJECT = 'pokoyakazan-test-01'\n",
    "BUCKET = 'pokoyakazan-test-01'\n",
    "REGION = 'us-central1'\n",
    "\n",
    "os.environ['BUCKET'] = BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration using Spark SQL of Dataproc\n",
    "- 以下の分析はBQでもできるが今回はSpark SQLをGCSから読み込んで使ってみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Bays classification using Spark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flightsデータ読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparkで読み込むためのSchemaを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up schema to read in the CSV files on GCS\n",
    "from pyspark.sql.types import StringType, FloatType, StructType, StructField\n",
    "\n",
    "header = 'FL_DATE,UNIQUE_CARRIER,AIRLINE_ID,CARRIER,FL_NUM,ORIGIN_AIRPORT_ID,ORIGIN_AIRPORT_SEQ_ID,ORIGIN_CITY_MARKET_ID,ORIGIN,DEST_AIRPORT_ID,DEST_AIRPORT_SEQ_ID,DEST_CITY_MARKET_ID,DEST,CRS_DEP_TIME,DEP_TIME,DEP_DELAY,TAXI_OUT,WHEELS_OFF,WHEELS_ON,TAXI_IN,CRS_ARR_TIME,ARR_TIME,ARR_DELAY,CANCELLED,CANCELLATION_CODE,DIVERTED,DISTANCE,DEP_AIRPORT_LAT,DEP_AIRPORT_LON,DEP_AIRPORT_TZOFFSET,ARR_AIRPORT_LAT,ARR_AIRPORT_LON,ARR_AIRPORT_TZOFFSET,EVENT,NOTIFY_TIME'\n",
    "\n",
    "def get_structfield(colname):\n",
    "  if colname in ['ARR_DELAY', 'DEP_DELAY', 'DISTANCE']:\n",
    "    return StructField(colname, FloatType(), True)\n",
    "  else:\n",
    "    return StructField(colname, StringType(), True)\n",
    "  \n",
    "schema = StructType([get_structfield(colname) for colname in header.split(',')])\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> ARR_DELAY', 'DEP_DELAY', 'DISTANCE'以外はStringTypeになっている"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SchemaとGCS上のcsvのPathを指定してflightsデータを読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table definition (this is done lazily; the files won't be read until we issue a query)\n",
    "#inputs = 'gs://{}/flights/tzcorr/all_flights-00000-*'.format(BUCKET)\n",
    "inputs = 'gs://{}/flights/tzcorr/all_flights-*'.format(BUCKET)\n",
    "flights_csv = spark.read\\\n",
    "            .schema(schema)\\\n",
    "            .csv(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 読み込んだデータからTempView作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_csv.createOrReplaceTempView('flights_tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "results = spark.sql('SELECT COUNT(*) FROM flights_tmp WHERE dep_delay > -20 AND distance < 2000')\n",
    "results.show()\n",
    "results = spark.sql('SELECT * FROM flights_tmp WHERE dep_delay > -20 AND distance < 2000')\n",
    "results.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_dayデータ読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainday.csvは小さいデータなのでspark側にカラムごとの型を推測(inferSchema)させる\n",
    "- また、最初の一行が絡む名なのでheaderをtrue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = 'gs://{}/flights/trainday.csv'.format(BUCKET)\n",
    "traindays = spark.read \\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .csv(inputs)\n",
    "traindays.createOrReplaceTempView('traindays_tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "results = spark.sql('select * from traindays_tmp')\n",
    "results.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"\"\"\n",
    "select\n",
    " f.FL_DATE AS date,\n",
    " distance,\n",
    " dep_delay\n",
    "from flights_tmp f\n",
    "join traindays_tmp t\n",
    "on f.FL_DATE==t.FL_DATE\n",
    "where t.is_train_day\n",
    "and f.dep_delay is not null\n",
    "order by f.dep_delay desc\n",
    "\"\"\"\n",
    "train_flights = spark.sql(statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### トレーニングデータの統計情報確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"件数\"、\"フライト距離、出発遅延それぞれの最大値、平均、最小値\"など"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotするためにsparkデータフレームをpandasデータフレームに変換\n",
    "df = train_flights[(train_flights['distance'] < 2000)\n",
    "                   & (train_flights['dep_delay'] > -20)\n",
    "                   & (train_flights['dep_delay'] < 30)]\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hexbin plot で分布確認\n",
    "- フライト距離は250~500に集中\n",
    "- 出発遅延時間は-5~0に集中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfから2%をサンプリングする\n",
    "pdf = df.sample(False, 0.02, 20).toPandas() # to 100,000 rows approx on complete dataset\n",
    "g = sns.jointplot(pdf['distance'], pdf['dep_delay'], kind=\"hex\",\n",
    "                  size=10, joint_kws={'gridsize':20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ヒストブラムイコライゼーション"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データを均一に分ける境界値を見つける\n",
    "近似分位法を行ってデータ数が同じになるように間隔をきる\n",
    "- APPROX_QUANTILES\n",
    "  - BigQueryで近似分位法を行う関数\n",
    "- **approxQuantile** -> **今回はこっちを使用**\n",
    "  - sparkで近似分位法を行う関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# フライト距離の境界値\n",
    "distthresh = train_flights.approxQuantile('distance', list(np.arange(0, 1.0, 0.1)), 0.02)\n",
    "distthresh # 最初の値は最小値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出発遅延の境界値\n",
    "delaythresh = train_flights.approxQuantile('dep_delay', list(np.arange(0, 1.0, 0.1)), 0.05)\n",
    "delaythresh # 最初の値は最小値"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "すべてのフライトのおよそ1/10は1.0分から3.0分の出発遅延時間を持つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> **出発遅延時間が17分以上 かつ フライト距離が2419マイル以上のフライトは全体の1/100**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = spark.sql('SELECT COUNT(*) FROM flights_tmp WHERE dep_delay >= 17 AND distance >= 2419')\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 各セルにおいて時間通りに到着するフライトが70%以上かどうかを計算\n",
    "これ以降の計算はSparkでも可能だが、ここではPigを使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
